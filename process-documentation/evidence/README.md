![alt tag](https://github.com/AccentureFed/process-documentation/raw/master/agile-process-photos/response-images/proposal-header.png)

Below is an identical replica of our Attachment E submission.  To further elaborate on the evidence, we have provided a link to additional information reflecting images, videos, and documentation such as design mockups, to further demonstrate our understanding.


|#|criteria|evidence|supporting reference link|
|-------|---------------|------------------|------------------|
|a| assigned one leader, gave that person authority and responsibility, and held that person accountable for the quality of the prototype submitted|Upon RFQ receipt, a Product Owner (PO) was assigned. He had extensive expertise in Agile coaching putting him in a position to assess alternatives and weigh tradeoffs related to the prototype. He was given authority to allocate resources and funding to ensure the appropriate skill sets were available.   The team was aware the PO had the ultimate authority to define features, assign tasks and prioritize the backlog.  The PO worked with the team to characterize the prototype vision, "an interactive tool used to inform and educate consumers of food product recalls in an open or closed status" and associated features. The PO was actively involved daily in contributing and soliciting feedback at sprint reviews and during the retro. Daily he conducted backlog grooming and prioritized tasks to ensure the team was working on the most important tasks. User stories, defects and infrastructure needs were balanced and prioritized against user value and time.| [Evidence a](https://github.com/AccentureFed/process-documentation/tree/master/evidence/Attachment_E_Evidence_A) | 
|b| assembled a multidisciplinary and collaborative team including a minimum of 5 labor categories from the Development Pool labor categories to design and develop the prototype |In preparation for the RFQ, a dedicated team with the required skill set was defined to respond to the proposal.  Extenuating circumstances upon RFQ drop required us to re-define 100% of our resources.  Within 24 hours, our full multidisciplinary team had been assigned, covering eight labor categories -Agile Coach, Visual Designer, Interaction Designer/User Researcher/Usability Tester, Back End Developer, DevOps Engineer, Front End Developer, Product manager and Business Analyst.  Our Team’s Corporate reach back helped to address surge and evolving needs by tapping 3,800 US based Agile resources, 1,800 scrum masters and 800 DevOps practitioners.  Our proven experience executing Agile processes that has been applied to over 1,000 Agile Scrum, Kanban & Lean projects, spanning Federal & commercial projects, provided the framework and tools to quickly define sprint zero needs, and enable the teams to begin sprinting.  We strategically choose to keep our team small, 8 FTE, and run only a one feature team.  Our team was formed with the appropriate skill sets to define, build and test quality software.  Our team was empowered, self-organized and accountable for delivering value daily within each sprint. Our team did not have the luxury of each member dedicating 100% of their time to the RFQ.  Some roles were shared as team members came in and out.  With this, it was imperative that the team openly communicate, share dependencies, and resolve impediments together. | [Evidence b](https://github.com/AccentureFed/process-documentation/tree/master/evidence/Attachment_E_Evidence_B) | 
|c| understand what people need, by including people in the prototype development and design process |Our team took a twofold approach focusing (1) on the end-user and (2) on our own development team to ensure people’s needs were accounted for through development.  To meet the needs of our defined end-users Personas, our front and back end development was driven by User Centered Design practices and methodologies. Our PO worked to define the prototype vision, the team crafted three personas, Ann Jordan a Mom, Steven Jones an FDA Administrator and Kathy Miller a Nutritionist, to define the user stories that drive functionality. Our PO provided clarification and feedback at each Sprint review and demo.  A field survey with four people at a local grocery store, defined prototype functionality requirements with real world users’ needs and concerns expressed.  Wire frames and design mock ups evolved and were modified from feedback through surveys, usability testing, Persona development, sprint reviews, UAT, and 508 testing. To ensure clarity and priority for our development team, daily ceremonial activities such as Sprint reviews, Backlog grooming and prioritization where the KANBAN board was updated, occurred on a regular cadence (daily).  These events promoted open dialogue and a venue to discuss success, impediments, and collaborate. |[Evidence c](https://github.com/AccentureFed/process-documentation/tree/master/evidence/Attachment_E_Evidence_C)| 
|d| used at least three "human-centered design" techniques or tools |Human Centered Design stood as the backbone of our prototype.  The end user desiring information pertinent to FDA Food Recalls, their needs, wants and desires drove a prototype tailored uniquely to them. The First technique used, Understand User Context.  Three personas were developed to depict multiple user bases for the prototype, a Mom searching for information for her children, a Nutritionist looking for updates for her client base and an FDA Administrator monitoring site metrics and updating public information. Second technique, understand the aspirations of the end user. User stories describing the type of user, what they want and why with acceptance criteria was defined.  A survey was conducted at a local grocery store with four people matching our Personas to further understand functionality they envisioned.  Heavy PO engagement ensured their requirements were incorporated.  Technique Three, Implement Rapid Prototyping.  Once our understanding evolved, rapid prototyping allowed for design iterations and quick changes driven by continual feedback from the PO, Usability and 508 testing and the team.  Tasks were created and prioritized to ensure the most important work was being done to respond to user needs. Technique four, Test and Evaluate. We conducted usability testing with local and remote testers along with remote observers using a usability tool to allow collaboration between test administrators and remote observers. 508 and Accessibility testing using JAWS screen reader, the WAT tool bar and Keyboard Accessibility helped drive continuous improvement and enhance usability for disabled users.| [Evidence d](https://github.com/AccentureFed/process-documentation/tree/master/evidence/Attachment_E_Evidence_D)|  
|e| created or used a design style guide and/or a pattern library |To ensure our prototype was consistent and easy to maintain, the team utilized pattern libraries and created a light weight design guide to drive the UX and design patterns.  With the focus to put the user first through our prototype development, the pattern library allowed for consistency in our user experience on multiple device resolutions and orientations. Bootstrap was used to develop our responsive multi-device framework.  The team had two UX designers, one remote and one with an onsite presence with the team. Applying pattern libraries such as Font Awesome, Bootstrap UI and our custom design guide allowed consistency among our designers and mitigated a fragmented user experience.  Developing and implementing Pattern libraries and UI libraries also allowed the team to have functionality and design elements that could be reused, and standard with 508, keeping cost and time at a minimum. Collective Code Ownership was encouraged by establishing a pattern library and style guide to help provide consistency as elements were coded in a common way, making it easier for our front end developers to work across the code base.| [Evidence e](https://github.com/AccentureFed/process-documentation/tree/master/evidence/Attachment_E_Evidence_E) | 
|f| performed usability tests with people |Usability testing was implemented in a series of steps.   Our experienced UX designers used their knowledge and experience to take the prototype vision from our Product owner, defined personas and feature map to create our original design concept.  Multiple iterations were created through moc-ups and wire frame development.  At the daily sprint review, designs were reviewed and feedback from the development team, product owner and stakeholders was incorporated to subsequent designs.  Within 48 hours of the initial product direction, a solid design was conceptualized in an interactive, clickable wireframe. In 72 hours, usability test cases were created and testing was administered with real users against that design with individuals matching the personas created for the prototype.  Local and remote testing was conducted. Based on feedback received, the designs were refined and iterations made. 508 testing was done utilizing JAWS, the Wat tool bar and Keyboard Accessibility.  Results highlighted areas of our implemented design were not conducive to individuals with disabilities.  These critical tests allowed our team to ensure our prototype vision not only met the desires of our user base, but was accessible for all users.|[Evidence f](https://github.com/AccentureFed/process-documentation/tree/master/evidence/Attachment_E_Evidence_F)| 
|g| used an iterative approach, where feedback informed subsequent work or versions of the prototype | Following Agile best practices and utilizing the ceremonial engagements allowed our team to employ an iterative approach that was driven by feedback provided by the Product Owner, user and or through usability testing.  Daily Sprint reviews provided a working demo of the prototype and or review of documentation such as wire frames for the PO, development team and other stakeholders to view and give feedback. Local and remote Usability testing and 508 testing, drove subsequent changes and additional features to be added to the backlog.  Once new stories, features, defects, functionality or change requests had been captured via stickies on the KANBAN board, the product owner would prioritize the backlog based on user impact, time and value.  This prioritization enabled the PO during our daily sprint planning to set the sprint goals, and allow the team to define how those goals would be implemented and what could be taken on during the sprint. Feedback received during 508 testing had a higher user impact and value to the prototype user than small design changes driving those higher in the backlog to be worked. Our Agile approach ensured a constant feedback mechanism and opportunity to weigh new information against existing tasks thus driving dynamic versions of the prototype.|[Evidence g](https://github.com/AccentureFed/process-documentation/tree/master/evidence/Attachment_E_Evidence_G) | 
|h| created a prototype that works on multiple devices, and presents a responsive design |The vision of our Prototype was to be responsive, interactive and viewable from multiple devices. Our personas described a set of users where the solution had to be versatile and seamlessly support a variety of end-user devices. Using responsive design techniques, the UX designers focused on crafting each targeted device to provide optimal viewing and interactive experiences. Users are presented not only different layouts based on devices but the content also adapts to specific device types and their orientation. Users of higher resolution devices (Desktop) will be presented with content that is suited to the user’s behavior of perusing large data sets and research type patterns. Users on their mobile devices using the same application will be presented with content and layout that is conducive to their mobility behavior enabling quick access to critical information. Bootstrap responsive libraries were used to support auto sizing to minimize panning, resizing and scrolling. CSS3 was used to manage content responsiveness, sizing, and styling and overall design patterns of the application. |[Evidence h](https://github.com/AccentureFed/process-documentation/tree/master/evidence/Attachment_E_Evidence_H)| 
|i| used at least five modern and open-source technologies, regardless of architectural layer (frontend, backend, etc.) |X|[Evidence i](https://github.com/AccentureFed/process-documentation/tree/master/evidence/Attachment_E_Evidence_I)| 
|j| deployed the prototype on an Infrastructure as a Service (IaaS) or Platform as a Service (PaaS) provider, and indicated which provider they used |The production server is an IaaS with the provider being Amazon Web Service with an EC2 instance **where is the production server**. Jenkins was deploying to Tomcat on the EC2 instance. Each production deployment is a manual Jenkins Job build and deployment.  The team deployed at a minimum once a day after each new feature capability or bug fix was complete. Each team member had the credentials through Jenkins to push to production eliminating a single point of failure among the team.  The production server has an SSL certificate allowing open access. Our development environment initially was stood up as a standard IaaS with an EC2 instance with Tomcat installed.  Jenkins was continuously deploying on code commit.  The development environment did not have an SSL certificate, thus promoting an internal test environment deployed on each code commit. For traceability, the team modified the Grails build script allowing Jenkins to embed the job name, the build number, and the Git Commit hash into the war file.  This was done for both the deployment and production builds.  This enabled our testers to align defects to builds, and understand progression across the code. | [Evidence j](https://github.com/AccentureFed/process-documentation/tree/master/evidence/Attachment_E_Evidence_J) | 
|k| wrote unit tests for their code |For the backend development we utilized JUnit and Spock. Spock is the framework for Groovy and JUnit for Java. Each component for the backend received its own unit test, especially corner cases and null. Backend developers wrote unit tests simultaneously as they were developing.  Unit tests, over 30, were run on every build in Jenkins (NUMBER OF BUILDS). Jenkins ran all unit tests upon build.  Jenkins was configured to test for any code change every 15 minutes. If unit tests failed the build was not deployed and Jenkins alerted the team.  If defects and or bugs were identified, the developer would check the unit test to ensure it was correct. Bugs were logged and prioritized on the KANBAN board and committed to a sprint. For each defect that was corrected a unit test was written for that case. | [Evidence k](https://github.com/AccentureFed/process-documentation/tree/master/evidence/Attachment_E_Evidence_K)| 
|l| set up or used a continuous integration system to automate the running of tests and continuously deployed their code to their IaaS or PaaS provider |Jenkins was used as our continuous integration system. It was configured to test for code changes every 15 minutes in development. For production, a manual Jenkins build was required triggering the running of unit tests and a manual deployment to the production server. After a new change is pushed, Jenkins checks the "test" folder and runs all Unit tests for the project. **As of 7/1/2015, Jigsaw currently runs 80 java unit tests**. If all the tests pass, then Jenkins packages the repository into a Web Application Archive (WAR file). From there, the war file is deployed to our development server **at .** **As of 7/1/2015 at 11:00 am, there have been 90 builds **pushed to the development server.   If the tests failed for both production and development developers were instantly notified of build failure. For the development and production a separate Amazon Web Service EC2 instance was provisioned. Jenkins was continuously deploying on code commit. The development environment was hosted at http://52.7.254.19:8080/jenkins/job/Jigsaw-Dev/  The team utilized the GitHub integration. |[Evidence l](https://github.com/AccentureFed/process-documentation/tree/master/evidence/Attachment_E_Evidence_L) | 
|m| set up or used configuration management |Jenkins was used as our continuous integration system. It was configured to test for code changes every 15 minutes in development. For production, a manual Jenkins build was required triggering the running of unit tests and a manual deployment to the production server. After a new change is pushed, Jenkins checks the "test" folder and runs all Unit tests for the project. **As of 7/1/2015, Jigsaw currently runs 80 java unit tests**. If all the tests pass, then Jenkins packages the repository into a Web Application Archive (WAR file). From there, the war file is deployed to our development server **at .** **As of 7/1/2015 at 11:00 am, there have been 90 builds **pushed to the development server.   If the tests failed for both production and development developers were instantly notified of build failure. For the development and production a separate Amazon Web Service EC2 instance was provisioned. Jenkins was continuously deploying on code commit. The development environment was hosted at http://52.7.254.19:8080/jenkins/job/Jigsaw-Dev/  The team utilized the GitHub integration. |[Evidence m](https://github.com/AccentureFed/process-documentation/tree/master/evidence/Attachment_E_Evidence_M)| 
|n| set up or used continuous monitoring We achieved continuous monitoring by utilizing the AWS Elastic Load Balancer Health Check to monitor the health of our application nodes within an auto-scaling group. Through this process if an EC2 node becomes unresponsive we can immediately create a new one, eliminating down time. Our site demonstrates authentication with a username and password to prevent configuration changes and enables the ability to display metrics.  We established the security parameters on our Virtual Private Cloud hosted within AWS.  Thus, removing any of our EC2 instances from public accessibility.  Our site is only available through our elastic load balancer. We will only allow public access to our site, via SSL, over port 443 through our elastic load balancer. Our site does not store or distribute any PII, or classified data.  To monitor system health, a authenticated admin page was developed for an FDA administrator. This page allows, the Admin to create and publish alerts shown on the main site banner and view system health metrics.   |[Evidence n](https://github.com/AccentureFed/process-documentation/tree/master/evidence/Attachment_E_Evidence_N)| 
|o|deploy their software in a container (i.e., utilized operating-system-level virtualization) |The team choose to deploy our software into an Amazon Web Service EC2 instance for production running the Docker daemon. The application was deployed inside of a Docker container, from a Docker image running Ubuntu **X**, Tomcat **8** and Java 8. We choose Docker because it is open source, free and a modern application that is easy to use.  It also provides a fast and efficient way to deploy our application in a dynamic and responsive fashion. Our Docker container running on the Docker hypervisor is our operating system-level virtualization. | [Evidence o](https://github.com/AccentureFed/process-documentation/tree/master/evidence/Attachment_E_Evidence_O)| 
|p| provided sufficient documentation to install and run their prototype on another machine |X |[Evidence p](https://github.com/AccentureFed/process-documentation/tree/master/evidence/Attachment_E_Evidence_P)| 
|q| prototype and underlying platforms used to create and run the prototype are openly licensed and free of charge| X| [Evidence q](https://github.com/AccentureFed/process-documentation/tree/master/evidence/Attachment_E_Evidence_Q)| 
